{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a86700-be30-46f0-8ab7-2d99334d008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning\")\n",
    "sys.path=list(set(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1b78a-3c39-4139-ad5c-ffc73d674d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment=None\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(position=0,leave=True)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import utils\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298cd960-418a-4209-8d9c-31cbce502603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_read(df, model_name):\n",
    "    # df=pd.read_csv(os.path.join(output_dir , file_name))\n",
    "    true_y=df[\"True_label\"].values\n",
    "    pred_y=df[\"Predicted_label\"].values\n",
    "    pred_prob=df[\"Predicted_prob\"].values\n",
    "    best_threshold=df['best_threshold'].unique()[0]\n",
    "\n",
    "    # test_output=utils.model_evaluate(true_y.reshape(-1),pred_y)\n",
    "    test_output=utils.model_evaluate(true_y.reshape(-1),pred_prob,best_threshold)\n",
    "    metric=pd.DataFrame()\n",
    "    metric[\"model_type\"]=[f\"{model_name}\"]\n",
    "    metric[\"total complaint #\"]=[test_output[\"total positive\"]]\n",
    "    metric[\"false_positive\"]=[test_output[\"false positive\"]]\n",
    "    metric[\"false_negative\"]=[test_output[\"false_negative\"]]\n",
    "    metric[\"precision\"]=[test_output[\"precision\"]]\n",
    "    metric[\"recall\"]=[test_output[\"recall\"]]\n",
    "    metric[\"f1_score\"]=[test_output[\"f1_score\"]]\n",
    "    metric[\"roc_auc\"]=[test_output[\"AUC\"]]\n",
    "    metric[\"pr_auc\"]=[test_output[\"pr_auc\"]]\n",
    "    return metric\n",
    "\n",
    "def style_format(metrics, type=\"test set\"):\n",
    "    # metrics=metrics[metrics[\"model_type\"].apply(lambda x : x.split(\"-\")[0]==model.split(\"-\")[0])].reset_index(drop=True)\n",
    "    return metrics.style.format({\"total complaint #\":\"{:,}\",\"false_positive\":\"{:,}\",\"false_negative\":\"{:,}\", \"precision\":\"{:.2%}\", \"recall\":\"{:.2%}\", \\\n",
    "                                \"f1_score\":\"{:.2%}\", \"roc_auc\":\"{:.2%}\", \"pr_auc\":\"{:.2%}\"}) \\\n",
    "    .set_caption(f\"Performance Summary for {type} \") \\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [\n",
    "            ('color', 'red'),\n",
    "            ('font-size', '15px')\n",
    "        ]\n",
    "    }])\n",
    "\n",
    "def dist_func(df, cols):\n",
    "    tempt1=pd.DataFrame(df[cols].value_counts(dropna=False)).reset_index().rename(columns={'index':cols,cols:'count'})\n",
    "    tempt2=pd.DataFrame(df[cols].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':cols,cols:'percentage'})\n",
    "    tempt3=tempt1.merge(tempt2, on=cols, how=\"inner\")\n",
    "    tempt3=tempt3.loc[:,[cols,'count','percentage']]\n",
    "    return tempt3\n",
    "\n",
    "def style_format_dist(df,title):\n",
    "    return df.style.format({'count':'{:,}','percentage':'{:.2%}'})\\\n",
    "           .set_caption(f\"{title}\")\\\n",
    "           .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '12px')]}])\n",
    "\n",
    "def metrics_df_func(output_dir, model_name):\n",
    "    data_name=[x for x in os.listdir(output_dir) if x.split(\".\")[-1]==\"csv\"]\n",
    "    data_name=sorted(data_name)\n",
    "    df=pd.read_csv(os.path.join(output_dir , data_name[0]))\n",
    "    metrics=metrics_read(df,model_name)\n",
    "    N=data_name[0].split(\"_\")[1].split(\".\")[0]\n",
    "    metrics.insert(0,\"Recall in Val\",[f\"recall>={N}0%\"])\n",
    "    \n",
    "    for i in range(1,len(data_name)):\n",
    "        df=pd.read_csv(os.path.join(output_dir , data_name[i]))\n",
    "        tempt=metrics_read(df,model_name)\n",
    "        N=data_name[i].split(\"_\")[1].split(\".\")[0]\n",
    "        tempt.insert(0,\"Recall in Val\",[f\"recall>={N}%\"])\n",
    "        metrics=pd.concat([metrics,tempt],axis=0,ignore_index=True)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def metrics_df(output_dir, model_name):\n",
    "    data_name=[x for x in os.listdir(output_dir) if x.split(\".\")[-1]==\"csv\"]\n",
    "    data_name=sorted(data_name)\n",
    "    df=pd.read_csv(os.path.join(output_dir , data_name[0]))\n",
    "    metrics=metrics_read(df,model_name)\n",
    "    for i in range(1,len(data_name)):\n",
    "        df=pd.read_csv(os.path.join(output_dir , data_name[i]))\n",
    "        metrics=pd.concat([metrics,metrics_read(df,model_name)],axis=0,ignore_index=True)\n",
    "        \n",
    "    metrics.drop_duplicates(subset=[\"recall\"],inplace=True, keep=\"first\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def response_rate_eval(logit,label,topk):\n",
    "    DF=pd.DataFrame(columns=[\"pred_score\",\"actual_label\"])\n",
    "    DF[\"pred_score\"]=logit\n",
    "    DF[\"actual_label\"]=label\n",
    "    DF.sort_values(by=\"pred_score\", ascending=False, inplace=True)\n",
    "    response_rate={}\n",
    "    for p in topk:\n",
    "        N=math.ceil(int(DF.shape[0]*p))\n",
    "        DF2=DF.nlargest(N,\"pred_score\",keep=\"first\")\n",
    "        response_rate[str(int(p*100))+\"%\"]=DF2.actual_label.sum()/DF2.shape[0]\n",
    "    return response_rate\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "def bar_plot(data, colors=None, total_width=0.8, single_width=1, legend=True,title=None,subtitle=None,axis_truncation=0.5):\n",
    "    \"\"\"Draws a bar plot with multiple bars per data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        The axis we want to draw our plot on.\n",
    "\n",
    "    data: dictionary\n",
    "        A dictionary containing the data we want to plot. Keys are the names of the\n",
    "        data, the items is a list of the values.\n",
    "\n",
    "        Example:\n",
    "        data = {\n",
    "            \"x\":[1,2,3],\n",
    "            \"y\":[1,2,3],\n",
    "            \"z\":[1,2,3],\n",
    "        }\n",
    "\n",
    "    colors : array-like, optional\n",
    "        A list of colors which are used for the bars. If None, the colors\n",
    "        will be the standard matplotlib color cyle. (default: None)\n",
    "\n",
    "    total_width : float, optional, default: 0.8\n",
    "        The width of a bar group. 0.8 means that 80% of the x-axis is covered\n",
    "        by bars and 20% will be spaces between the bars.\n",
    "\n",
    "    single_width: float, optional, default: 1\n",
    "        The relative width of a single bar within a group. 1 means the bars\n",
    "        will touch eachother within a group, values less than 1 will make\n",
    "        these bars thinner.\n",
    "\n",
    "    legend: bool, optional, default: True\n",
    "        If this is set to true, a legend will be added to the axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if colors where provided, otherwhise use the default color cycle\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize =(15, 8))\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    \n",
    "    # Number of bars per group\n",
    "    n_bars = len(data)\n",
    "\n",
    "    # The width of a single bar\n",
    "    bar_width = total_width / n_bars\n",
    "\n",
    "    # List containing handles for the drawn bars, used for the legend\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over all data\n",
    "    for i, (name, values) in enumerate(data.items()):\n",
    "        # The offset in x direction of that bar\n",
    "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
    "\n",
    "        # Draw a bar for every value of that type\n",
    "        for x, y in enumerate(values.values()):\n",
    "            bar = ax.bar(x + x_offset, y, width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "\n",
    "        # Add a handle to the last drawn bar, which we'll need for the legend\n",
    "        bars.append(bar[0])\n",
    "\n",
    "    # Draw legend if we need\n",
    "    if legend:\n",
    "        ax.legend(bars, data.keys())\n",
    "    \n",
    "    ax.set_ylabel('Accuracy',fontsize=15)\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda y,_: \"{:.0%}\".format(y)))\n",
    "    ind=np.arange(len(data[list(data.keys())[0]]))\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels( ('top 1% score', 'top 2% score', 'top 5% score','top 10% score') )\n",
    "    ax.set_title(f\"Top Predicted Score  \",fontsize=15)\n",
    "    \n",
    "    #     plt.xlim([0, 1])\n",
    "    # plt.ylim([axis_truncation, 1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f2c5c-1ded-4a86-bd03-08ce68a0a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALL=[]\n",
    "PRECISION=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d3359-7b0a-40d8-bb53-26d849d938d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date=\"05_23\"\n",
    "number_feature=990\n",
    "data_name=\"test_data_\"+str(number_feature)\n",
    "input_data=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_data/{test_date}/{data_name}\"\n",
    "tst=pd.read_pickle(input_data)\n",
    "a, b=tst[\"target_variable\"].value_counts().tolist()\n",
    "print(\"Precision of human review : {:.2%}\".format(b/(a+b)))\n",
    "\n",
    "PRECISION.append(b/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf2638d-95b8-4627-9e56-279c30b337e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"lightgbm\"\n",
    "test_date=\"05_23\"\n",
    "number_feature=990\n",
    "\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_model/{test_date}/{number_feature}/{model_name}/\"\n",
    "metrics=metrics_df_func(output_dir, model_name)\n",
    "# metrics.drop_duplicates(subset=[\"recall\"],inplace=True, keep=\"first\")\n",
    "metrics.sort_values(\"recall\",inplace=True)\n",
    "metrics=metrics.reset_index(drop=True)\n",
    "metrics.loc[0,\"Recall in Val\"]=\"default\"\n",
    "\n",
    "# style_format(metrics,  type=f\"{model_name} model\")\n",
    "\n",
    "PRECISION.append(metrics[metrics['Recall in Val']==\"recall>=97%\"].precision.values[0])\n",
    "RECALL.append(metrics[metrics['Recall in Val']==\"recall>=97%\"].recall.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0637a7d-063a-4d73-bad9-2333f3032d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date=\"06_23\"\n",
    "number_feature=990\n",
    "data_name=\"test_data_\"+str(number_feature)\n",
    "input_data=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_data/{test_date}/{data_name}\"\n",
    "tst=pd.read_pickle(input_data)\n",
    "a, b=tst[\"target_variable\"].value_counts().tolist()\n",
    "print(\"Precision of human review : {:.2%}\".format(b/(a+b)))\n",
    "\n",
    "PRECISION.append(b/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95f740-7313-413f-9a6d-2ee62bff5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/inference_test_data/results/\"\n",
    "df=pd.read_csv(os.path.join(data_dir,\"predictions_06.csv\"))\n",
    "metrics=metrics_read(df, model_name=\"lightgbm-frozen-06\")\n",
    "RECALL.append(metrics.recall.values[0])\n",
    "PRECISION.append(metrics.precision.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50373a1-e712-43d8-a19f-c816bb6b2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"lightgbm\"\n",
    "test_date=\"06_23\"\n",
    "number_feature=990\n",
    "\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_model/{test_date}/{number_feature}/{model_name}/\"\n",
    "metrics=metrics_df_func(output_dir, model_name)\n",
    "# metrics.drop_duplicates(subset=[\"recall\"],inplace=True, keep=\"first\")\n",
    "metrics.sort_values(\"recall\",inplace=True)\n",
    "metrics=metrics.reset_index(drop=True)\n",
    "metrics.loc[0,\"Recall in Val\"]=\"default\"\n",
    "\n",
    "# style_format(metrics,  type=f\"{model_name} model\")\n",
    "PRECISION.append(metrics[metrics['Recall in Val']==\"recall>=97%\"].precision.values[0])\n",
    "RECALL.append(metrics[metrics['Recall in Val']==\"recall>=97%\"].recall.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d8ac0-1141-4759-8487-e74cbf76cbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60838c-f649-4cb4-8ed0-155c71bec5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date=\"07_23\"\n",
    "number_feature=990\n",
    "data_name=\"test_data_\"+str(number_feature)\n",
    "input_data=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_data/{test_date}/{data_name}\"\n",
    "tst=pd.read_pickle(input_data)\n",
    "a, b=tst[\"target_variable\"].value_counts().tolist()\n",
    "print(\"Precision of human review : {:.2%}\".format(b/(a+b)))\n",
    "\n",
    "PRECISION.append(b/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a2aa1-3820-4662-93c8-dac18d330320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/inference_test_data/results/\"\n",
    "df=pd.read_csv(os.path.join(data_dir,\"predictions_07.csv\"))\n",
    "metrics=metrics_read(df, model_name=\"lightgbm-frozen-07\")\n",
    "RECALL.append(metrics.recall.values[0])\n",
    "PRECISION.append(metrics.precision.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a4637-af50-4da1-81ea-0e7b4ed43a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"lightgbm\"\n",
    "test_date=\"07_23\"\n",
    "number_feature=990\n",
    "\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_model/{test_date}/{number_feature}/{model_name}/\"\n",
    "metrics=metrics_df_func(output_dir, model_name)\n",
    "# metrics.drop_duplicates(subset=[\"recall\"],inplace=True, keep=\"first\")\n",
    "metrics.sort_values(\"recall\",inplace=True)\n",
    "metrics=metrics.reset_index(drop=True)\n",
    "metrics.loc[0,\"Recall in Val\"]=\"default\"\n",
    "\n",
    "# style_format(metrics,  type=f\"{model_name} model\")\n",
    "\n",
    "PRECISION.append(metrics[metrics['Recall in Val']==\"recall>=97%\"].precision.values[0])\n",
    "RECALL.append(metrics[metrics['Recall in Val']==\"recall>=97%\"].recall.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3d7ad-7990-417d-b1e6-b360485949d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recall=pd.DataFrame({\"model\":[\"TFIDF-ligtgbm\",\"TFIDF-ligtgbm-frozen\",\"TFIDF-ligtgbm-dynamic\",\"TFIDF-ligtgbm-frozen\",\"TFIDF-ligtgbm-dynamic\"],\n",
    "                        \"date\":[\"05/2023\",  \"06/2023\", \"06/2023\", \"07/2023\", \"07/2023\"],\n",
    "                        \"Recall\":RECALL\n",
    "                       })\n",
    "\n",
    "colors = {\"TFIDF-ligtgbm\": \"red\", \"TFIDF-ligtgbm-frozen\": \"green\", \"TFIDF-ligtgbm-dynamic\": \"purple\"}\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "dates = df_recall['date'].unique()\n",
    "width = 0.2\n",
    "for idx, date in enumerate(dates):\n",
    "    subset = df_recall[df_recall['date'] == date]\n",
    "    for j, (model, precision) in enumerate(zip(subset['model'], subset['Recall'])):\n",
    "        plt.bar(idx + j*width, precision, width, color=colors[model])\n",
    "        plt.annotate(f\"{precision*100:.2f}%\",\n",
    "                     (idx + j*width, precision),\n",
    "                     ha='center', va='bottom',\n",
    "                     fontsize=10, color='black')\n",
    "\n",
    "# Adjust x ticks and labels\n",
    "plt.xticks([i + 0.3 for i in range(len(dates))], dates)\n",
    "\n",
    "# Explicitly setting the legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [Line2D([0], [0], color=color, lw=4) for model, color in colors.items()]\n",
    "plt.legend(legend_handles, colors.keys(), title='Model', loc='upper right')\n",
    "\n",
    "plt.title('Recall of Different Models Over Time')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Date')\n",
    "plt.ylim([0.6,1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fe3a2-3ee7-4adc-a3c9-bb49c5ff63cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b51a64-88be-40e8-bcfb-1338639ab769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precision=pd.DataFrame({\"model\":[\"human-review\",\"TFIDF-ligtgbm\",\"human-review\",\"TFIDF-ligtgbm-frozen\",\"TFIDF-ligtgbm-dynamic\",\"human-review\",\"TFIDF-ligtgbm-frozen\",\"TFIDF-ligtgbm-dynamic\"],\n",
    "                        \"date\":[\"05/2023\",\"05/2023\",\"06/2023\",\"06/2023\",\"06/2023\",\"07/2023\",\"07/2023\",\"07/2023\"],\n",
    "                        \"precision\":PRECISION\n",
    "                       })\n",
    "\n",
    "colors = {\"human-review\": \"blue\", \"TFIDF-ligtgbm\": \"red\", \"TFIDF-ligtgbm-frozen\": \"green\", \"TFIDF-ligtgbm-dynamic\": \"purple\"}\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "dates = df_precision['date'].unique()\n",
    "width = 0.2\n",
    "for idx, date in enumerate(dates):\n",
    "    subset = df_precision[df_precision['date'] == date]\n",
    "    for j, (model, precision) in enumerate(zip(subset['model'], subset['precision'])):\n",
    "        plt.bar(idx + j*width, precision, width, color=colors[model])\n",
    "        plt.annotate(f\"{precision*100:.2f}%\",\n",
    "                     (idx + j*width, precision),\n",
    "                     ha='center', va='bottom',\n",
    "                     fontsize=10, color='black')\n",
    "\n",
    "# Adjust x ticks and labels\n",
    "plt.xticks([i + 0.3 for i in range(len(dates))], dates)\n",
    "\n",
    "# Explicitly setting the legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [Line2D([0], [0], color=color, lw=4) for model, color in colors.items()]\n",
    "plt.legend(legend_handles, colors.keys(), title='Model', loc='upper right')\n",
    "\n",
    "plt.title('Precision of Different Models Over Time')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc462d-bb00-4d3d-9003-3c4e2bfb07e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a0cbd-0070-4b1a-8867-e9eda414a935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c72872-1b66-4764-bf39-c370a9642075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f1681-38b2-46fe-8732-4ae85cc9e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Define precision and recall values for each model\n",
    "# models = ['TFIDF_number_500','TFIDF_number_750','TFIDF_number_1000','TFIDF_number_5000']\n",
    "# markers = ['o', 's', 'D','x']\n",
    "# colors = ['blue', 'orange','green','red']\n",
    "\n",
    "models = ['TFIDF_number_500','TFIDF_number_1000','TFIDF_number_5000']\n",
    "markers = ['o', 's','x']\n",
    "colors = ['blue', 'orange','red']\n",
    "\n",
    "\n",
    "# Plot precision and recall\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Iterate over models\n",
    "for i in range(len(models)):\n",
    "    plt.plot(recall[i], precision[i], marker=markers[i],  color=colors[i], label=models[i], linewidth=3, linestyle=\":\", markersize=8)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.title('Precision-Recall Curve \\n(test_set=05/2023)', fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "# Format axis values as percentages\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mtick.MultipleLocator(base=0.01))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax.yaxis.set_major_locator(mtick.MultipleLocator(base=0.001))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=2))\n",
    "\n",
    "plt.ylim(0.008,0.015)\n",
    "plt.xlim(0.90,1.001)\n",
    "\n",
    "# Add horizontal line for benchmark model\n",
    "# benchmark_precision=0.0053\n",
    "# plt.axhline(y=benchmark_precision, color=(0.8,0.7,0.5),linestyle='--', linewidth=3)\n",
    "\n",
    "# Set the legend\n",
    "plt.legend()\n",
    "# plt.legend(models+[\"Lexican Search\"],bbox_to_anchor=(1,0.5), fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e79ce9-377c-4058-9dde-66b899afeff0",
   "metadata": {},
   "source": [
    "#### longformer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254f8ce-2144-4ece-b484-7203889b441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_base_4096_customized\"\n",
    "test_date=\"05_23\"\n",
    "\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "model_name=\"longformer_base\"\n",
    "metrics=metrics_df_func(output_dir, model_name)\n",
    "metrics.drop_duplicates(subset=[\"recall\"],inplace=True, keep=\"first\")\n",
    "style_format(metrics,  type=\"longformer_base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc245a6-4c13-414e-ba8a-f359d986d097",
   "metadata": {},
   "source": [
    "#### longformer large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11379b75-f4dc-4fcd-aad9-4dc487b4774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large_4096_customized\"\n",
    "test_date=\"05_23\"\n",
    "\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "model_name=\"longformer_large\"\n",
    "metrics=metrics_df_func(output_dir, model_name)\n",
    "metrics.drop_duplicates(subset=[\"recall\"],inplace=True, keep=\"first\")\n",
    "style_format(metrics,  type=\"longformer_base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88ef89-c39c-47e2-8538-ecead405d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=[]\n",
    "recall=[]\n",
    "\n",
    "test_date=\"05_23\"\n",
    "number_feature=995\n",
    "model_name=\"lightgbm\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_model/{test_date}/{number_feature}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"lightgbm\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"roberta_large_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"roberta_large\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"deberta_v3_large\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"deberta_v3\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_base_4096_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_base\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_large_4096_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_large\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"bigbird_roberta_large_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"bigbird\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcf079-de53-481c-877a-02342af2b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Define precision and recall values for each model\n",
    "# models = ['TFIDF_lightgbm','Roberta-Large','Deberta-v3','longformer-base','longformer-large', 'bigbird']\n",
    "# markers = ['o', 's', 'x', '*', '<', 'p']\n",
    "# colors = ['blue', 'green','purple','red','black','brown']\n",
    "\n",
    "models = ['TFIDF_lightgbm','Roberta-Large','Deberta-v3','longformer-base','longformer-large','bigbird']\n",
    "markers = ['o', 's', 'x', '*', '<','D']\n",
    "colors = ['blue', 'green','purple','red','black','orange']\n",
    "\n",
    "# Plot precision and recall\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Iterate over models\n",
    "for i in range(len(models)):\n",
    "    plt.plot(recall[i], precision[i], marker=markers[i],  color=colors[i], label=models[i], linewidth=3, linestyle=\":\", markersize=8)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.title('Precision-Recall Curve \\n(test_set=05/2023)', fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "# Format axis values as percentages\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mtick.MultipleLocator(base=0.01))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax.yaxis.set_major_locator(mtick.MultipleLocator(base=0.001))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=2))\n",
    "\n",
    "plt.ylim(0.006,0.03)\n",
    "plt.xlim(0.96,1.001)\n",
    "\n",
    "# Add horizontal line for benchmark model\n",
    "benchmark_precision=0.0074\n",
    "plt.axhline(y=benchmark_precision, color=(0.8,0.7,0.5),linestyle='--', linewidth=3)\n",
    "\n",
    "# Set the legend\n",
    "plt.legend(loc='upper left')\n",
    "plt.legend(models+[\"Lexican Search\"],bbox_to_anchor=(1.35,0.5), fontsize=14)\n",
    "# plt.legend(models+[\"Lexican Search\"], fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843bf98-b300-43fe-9d59-e54515dfe1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56054a-5f62-4dba-b05b-3cee489b059f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25c2b9-3497-43eb-af88-b3d84cbd220f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec669a-30f3-4d9b-b4a6-ac43eccd555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=[]\n",
    "recall=[]\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"roberta_large\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"roberta_large\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"roberta_large_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"roberta_large_customized\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_base_4096\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_base\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_base_4096_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_base_customized\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_large_4096\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_large\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_large_4096_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_large_customized\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"bigbird_roberta_large\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"bigbird\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"bigbird_roberta_large_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"bigbird_customized\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Define precision and recall values for each model\n",
    "models = ['Roberta-Large','Roberta-Large-customized','longformer-base',\\\n",
    "          'longformer-base-customized','longformer-large', 'longformer-large-customized',\n",
    "          'bigbird', 'bigbird-customized']\n",
    "\n",
    "markers = ['o', 's', 'x', '*', '<', 'p','>','D']\n",
    "colors = ['blue', 'green','purple','red','black','brown','lawngreen','orange']\n",
    "\n",
    "# Plot precision and recall\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Iterate over models\n",
    "for i in range(len(models)):\n",
    "    plt.plot(recall[i], precision[i], marker=markers[i],  color=colors[i], label=models[i], linewidth=3, linestyle=\":\", markersize=8)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.title('Precision-Recall Curve \\n(test_set=05/2023)', fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "# Format axis values as percentages\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mtick.MultipleLocator(base=0.01))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax.yaxis.set_major_locator(mtick.MultipleLocator(base=0.001))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=2))\n",
    "\n",
    "plt.ylim(0.006,0.03)\n",
    "plt.xlim(0.96,1.001)\n",
    "\n",
    "# Add horizontal line for benchmark model\n",
    "benchmark_precision=0.0074\n",
    "plt.axhline(y=benchmark_precision, color=(0.8,0.7,0.5),linestyle='--', linewidth=3)\n",
    "\n",
    "# Set the legend\n",
    "plt.legend(loc='upper left')\n",
    "plt.legend(models+[\"Lexican Search\"],bbox_to_anchor=(1,0.5), fontsize=14)\n",
    "# plt.legend(models+[\"Lexican Search\"], fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c9780-fc01-4cb8-ba5b-15c09f9a5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=metrics.groupby(\"model_type\")[\"recall\"].idxmax()\n",
    "# metrics_2=metrics.loc[idx]\n",
    "# desired_order=[\"randomforest\",\"xgboost\",\"lightgbm\",\"roberta_large\",\"deberta_v3_large\",\"longformer_base\",\"longformer_large\",\"bigbird\"]\n",
    "# metrics_2[\"model_type\"]=pd.Categorical(metrics_2[\"model_type\"],categories=desired_order,ordered=True)\n",
    "# metrics_2=metrics_2.sort_values(by=\"model_type\")\n",
    "# style_format(metrics_2,  type=\"Different Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962d889-ab79-4a48-8f08-aeeb66dcfc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0691e-102d-4deb-a1d8-f1f05e2c9050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7253b-da82-449d-b2ea-2dcefb2d8b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de3467-ea82-4e70-9b85-949f3a2b357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=[]\n",
    "recall=[]\n",
    "\n",
    "test_date=\"05_23\"\n",
    "number_feature=995\n",
    "model_name=\"lightgbm\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/TFIDF/production/tfidf_model/{test_date}/{number_feature}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"lightgbm\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "\n",
    "test_date=\"05_23\"\n",
    "model_name=\"longformer_base_4096_customized\"\n",
    "output_dir=f\"/opt/omniai/work/instance1/jupyter/v5_new_email/Fine-Tuning/results/{test_date}/{model_name}/\"\n",
    "df=metrics_df(output_dir, \"longformer_base\")\n",
    "df=df[df.recall>0.9]\n",
    "precision.append(df[\"precision\"].tolist())\n",
    "recall.append(df[\"recall\"].tolist())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Define precision and recall values for each model\n",
    "# models = ['TFIDF_lightgbm','Roberta-Large','Deberta-v3','longformer-base','longformer-large', 'bigbird']\n",
    "# markers = ['o', 's', 'x', '*', '<', 'p']\n",
    "# colors = ['blue', 'green','purple','red','black','brown']\n",
    "\n",
    "models = ['TFIDF_model','longformer-base']\n",
    "markers = ['o', 'D']\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "# Plot precision and recall\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Iterate over models\n",
    "for i in range(len(models)):\n",
    "    plt.plot(recall[i], precision[i], marker=markers[i],  color=colors[i], label=models[i], linewidth=3, linestyle=\":\", markersize=8)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.title('Precision-Recall Curve \\n(test_set=05/2023)', fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "# Format axis values as percentages\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mtick.MultipleLocator(base=0.01))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax.yaxis.set_major_locator(mtick.MultipleLocator(base=0.001))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=2))\n",
    "\n",
    "plt.ylim(0.006,0.03)\n",
    "plt.xlim(0.96,1.001)\n",
    "\n",
    "# Add horizontal line for benchmark model\n",
    "benchmark_precision=0.0074\n",
    "plt.axhline(y=benchmark_precision, color=(0.8,0.7,0.5),linestyle='--', linewidth=3)\n",
    "\n",
    "# Set the legend\n",
    "plt.legend(loc='upper left')\n",
    "plt.legend(models+[\"Lexican Search\"],bbox_to_anchor=(1.35,0.5), fontsize=14)\n",
    "# plt.legend(models+[\"Lexican Search\"], fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb30ec-0d17-44f7-877b-1f0cf42e138b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt0",
   "language": "python",
   "name": "pt0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
